<!DOCTYPE html>
<!--

-->

<html>
 	<head>
		<title>Enrique Dunn Web Page</title>
		<meta charset=utf-8 >
		<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
		<meta name="robots" content="index, follow" >
		<meta name="keywords" content="Dunn, 3D Computer Vision" >
		<meta name="description" content="Enrique Dunn Research Page " >
		<meta name="author" content="Enrique Dunn">

		<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

		<!-- FAVICONS -->
  <link rel="apple-touch-icon" sizes="57x57" href="images/favicons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="images/favicons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="images/favicons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="images/favicons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="images/favicons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="images/favicons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="images/favicons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="images/favicons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="images/favicons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="images/favicons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="images/favicons/favicon-194x194.png" sizes="194x194">
  <link rel="icon" type="image/png" href="images/favicons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="images/favicons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="images/favicons/favicon-16x16.png" sizes="16x16">
<!-- CSS -->

    <!-- BOOTSTRAP -->
		<link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- GOOGLE FONT -->
		<link href='https://fonts.googleapis.com/css?family=Lato:300,400,700%7COpen+Sans:400,300,700' rel='stylesheet' type='text/css'>

    <!-- ICONS ELEGANT FONT & FONT AWESOME & LINEA ICONS -->
		<link rel="stylesheet" href="css/icons-fonts.css" >

    <!-- CSS THEME -->
		<link rel="stylesheet" href="css/style.css" >

    <!-- ANIMATE -->
		<link rel='stylesheet' href="css/animate.min.css">

    <!-- IE Warning CSS -->
		<!--[if lte IE 8]><link rel="stylesheet" type="text/css" href="css/ie-warning.css" ><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" type="text/css" href="css/ie8-fix.css" ><![endif]-->

    <!-- Magnific popup  in style.css	Owl Carousel Assets in style.css -->

<!-- CSS end -->

<!-- JS begin some js files in bottom of file-->

		<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
		<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
		<!--[if lt IE 9]>
		  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
		  <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    <!-- Modernizr -->
    <!-- <script src="js/modernizr.js"></script> -->

	</head>
	<body>

		<!-- LOADER -->
		<div id="loader-overflow">
          <div id="loader3">Please enable JS</div>
        </div>

		<div id="wrap" class="boxed ">
            <div class="grey-bg"> <!-- Grey BG  -->

				<!--[if lte IE 8]>
				<div id="ie-container">
					<div id="ie-cont-close">
						<a href='#' onclick='javascript&#058;this.parentNode.parentNode.style.display="none"; return false;'><img src='images/ie-warn/ie-warning-close.jpg' style='border: none;' alt='Close'></a>
					</div>
					<div id="ie-cont-content" >
						<div id="ie-cont-warning">
							<img src='images/ie-warn/ie-warning.jpg' alt='Warning!'>
						</div>
						<div id="ie-cont-text" >
							<div id="ie-text-bold">
								You are using an outdated browser
							</div>
							<div id="ie-text">
								For a better experience using this site, please upgrade to a modern web browser.
							</div>
						</div>
						<div id="ie-cont-brows" >
							<a href='http://www.firefox.com' target='_blank'><img src='images/ie-warn/ie-warning-firefox.jpg' alt='Download Firefox'></a>
							<a href='http://www.opera.com/download/' target='_blank'><img src='images/ie-warn/ie-warning-opera.jpg' alt='Download Opera'></a>
							<a href='http://www.apple.com/safari/download/' target='_blank'><img src='images/ie-warn/ie-warning-safari.jpg' alt='Download Safari'></a>
							<a href='http://www.google.com/chrome' target='_blank'><img src='images/ie-warn/ie-warning-chrome.jpg' alt='Download Google Chrome'></a>
						</div>
					</div>
				</div>
				<![endif]-->

      <div class="header-transporent-bg-black">

				<!-- HEADER -->
                <header id="nav" class="header header-1 black-header affix-on-mobile">
<!--				<header id="nav" class="header header-1 black-header">-->
				  <div class="header-wrapper">
					<div class="container-m-30 clearfix">
					  <div class="logo-row">

						<!-- LOGO -->
						<div class="logo-container-2">
                            <div class="logo-2">
                              <a href="https://www.stevens.edu" class="clearfix">
                                <img src="images/StevensLogoWhite.png" class="logo-img" alt="Logo">
                              </a>
                            </div>
                          </div>
						<!-- BUTTON -->
						<div class="menu-btn-respons-container">
							<button type="button" class="navbar-toggle btn-navbar collapsed" data-toggle="collapse" data-target="#main-menu .navbar-collapse">
								<span aria-hidden="true" class="icon_menu hamb-mob-icon"></span>
							</button>
						</div>
					 </div>
					</div>

					<!-- MAIN MENU CONTAINER -->
					<div class="main-menu-container">
					  <div class="container-m-20 clearfix">
							<!-- MAIN MENU -->
							<div id="main-menu">
							  <div class="navbar navbar-default" role="navigation">
                                  <!-- MAIN MENU LIST -->
                                  <nav class="collapse collapsing navbar-collapse nav-center">
                                    <ul id="nav-onepage" class="nav navbar-nav">
                                          <!-- MENU ITEM -->
                                          <li class="current">
                                            <a href="#home-link"><div class="main-menu-title">HOME</div></a>
                                          </li>
                                          <!-- MENU ITEM -->
                                          <li>
                                            <a href="#team-link"><div class="main-menu-title">TEAM</div></a>
                                          </li>
                                          <!-- MENU ITEM -->
                                          <li>
                                            <a href="#news-link"><div class="main-menu-title">NEWS</div></a>
                                          </li>
                                          <!-- MENU ITEM -->

                                          <li>
                                            <a href="#research-link"><div class="main-menu-title">RESEARCH</div></a>
                                          </li>


                                          <!-- MENU ITEM -->
                                          <li>
                                            <a href="#papers-link"><div class="main-menu-title">PAPERS</div></a>
                                          </li>
                                          <!-- MENU ITEM -->
                                          <li>
                                            <a href="#bio-link"><div class="main-menu-title">CV</div></a>
                                          </li>
                                          <!-- MENU ITEM -->
                                          <li>
                                            <a href="#contact-link"><div class="main-menu-title">CONTACT</div></a>
                                          </li>
                                    </ul>
                                  </nav>
                              </div>
                            </div>
              <!-- END main-menu -->

                        </div>
            <!-- END container-m-30 -->

					</div>
					<!-- END main-menu-container -->

					<!-- SEARCH READ DOCUMENTATION -->
					<ul class="cd-header-buttons">
						<li><a class="cd-search-trigger" href="#cd-search"><span></span></a></li>
					</ul> <!-- cd-header-buttons -->
					<div id="cd-search" class="cd-search">
						<form class="form-search" id="searchForm" action="page-search-results.html" method="get">
							<input type="text" value="" name="q" id="q" placeholder="Search...">
						</form>
					</div>

				  </div>
				  <!-- END header-wrapper -->

				</header>

        <!-- STATIC MEDIA IMAGE -->
        <div  class="sm-img-bg-fullscr parallax-section" style="background-image: url(images/ED_pic2.jpg)" data-stellar-background-ratio="0.75">
<!--        <div class="container sm-content-cont js-height-fullscr">-->
        <div class="container sm-content-cont ">
            <div class="sm-cont-middle">

							<!-- OPACITY container -->
							<div class="opacity-scroll2">

						  	<!-- LAYER NR. 1 -->
						  	<div class="font-white light-50-wide sm-mb-15 sm-mt-20" >
						      <span class="bold">Enrique Dunn</span><br>
				              <span class="bold">Associate Professor</span> <br>
				              <span class="bold">	Stevens Institute of Technology</span>
						  	</div>
							</div>

            </div>
          </div>
          <!-- SCROLL ICON -->
					<div class="local-scroll-cont font-white">
						<a href="#home-link" class="scroll-down smooth-scroll">
              <div class="icon icon-arrows-down"></div>
            </a>
					</div>
<!--        </div>    -->
        </div>

      </div>





        <!-- CALL TO ACTION  -->
<!--
        <div class="port-view-more-cont-dark">
          <a class="port-view-more-dark " href="contact.html">LET'S WORK TOGETHER</a>
        </div>
-->


        <div id="home-link" class="page-section pt-110-b-30-cont">
          <div class="container">
            <div class="mb-50">
<!--              <h2 class="section-title pr-0">Welcome... <span class="bold"></span>-->
<!--                  <a href="blog-right-sidebar.html" class="section-more right">BLOG</a>-->
<!--                  </h2>-->
                I'm an Associate Professor within the Department of Computer Science at the Stevens Institute of Techonology, carrying out computer vision research for the analysis and exploitation of visual data. In particular, I'm interested in studying the geometric and semantic relationships found among an imaged environment, the agents interacting within it, and the sensors observing them. Recently, my interest has been on  developing visual analytics for large-scale  and heterogeneous datasets, such as those comprised by crowd-source imagery.


            </div>
            <!--           </div>   -->

            <div id="team-link" class="mb-50">
              <h2 class="section-title pr-0">TEAM <span class="bold">MEMBERS</span></h2>
            </div>




            <!-- TEAM 1 -->
    				<div class=" pb-30">
    					<div class="container">
                <div class="row">

                  <div class="member col-md-2 col-sm-2 wow fadeInUp">
                    <div class="member-image">
                      <img src="images/Team-Xu2.png" alt="img">
                    </div>
                    <h3> <a  href="https://shawnxu10.github.io"> XIANGYU XU </a></h3>
                    <span>Ph.D. Student</span>
                  </div>



                  <div class="member col-md-2 col-sm-2 wow fadeInUp">
                    <div class="member-image">
                      <img src="images/Team-Min-Croppped.png" alt="img">
                    </div>
                    <h3><a  href="https://htkseason.github.io">ZHIXIANG  MIN</a></h3>
                    <span>Ph.D. Student</span>
                  </div>

                  <div class="member col-md-2 col-sm-2 wow fadeInUp">
                    <div class="member-image">
                      <img src="images/Team-Dibene-Croppped.png" alt="img">
                    </div>
                    <h3><a  href="https://jdibenes.github.io">JUAN CARLOS DIBENE</a> </h3>
                    <span>Ph.D. Student</span>
                  </div>

                  <div class="member col-md-2 col-sm-2 wow fadeInUp">
                    <div class="member-image">
                      <img src="images/Team-Cao-Croppped.png" alt="img">
                    </div>
                    <h3><a  href="https://siyuancao18.wixsite.com/tony">SIYUAN CAO</a> </h3>
                    <span>Ph.D. Student</span>
                  </div>









                </div>
    					</div>
            </div>





            <div id="news-link" class="mb-50">
              <h2 class="section-title pr-0">LATEST <span class="bold">NEWS</span>
                  </h2>
            </div>

           <div class="row">
<!--               <div class="col-md-2 blog2-post-title-cont">  </div>  <div class="col-md-10 blog2-post-title-cont">  -->

<!-- Post Item ICCV ORAL -->
  <div class="col-md-12 wow fadeIn pb-30" >

    <div class="row">
      <div class="col-md-4 blog2-post-title-cont">
        <div class="post-prev-date-cont">
          <span class="blog2-date-numb">    2021</span><span class="blog2-month">JULY</span>
        </div>
        <div class="post-prev-title">
          <h3><a href="">ICCV 2021 Oral</a></h3>
          <div class="post-prev-info">

          </div>
        </div>
      </div>

      <div class="col-md-8">
        <div class="blog2-post-prev-text">
          Our paper "<span class="bold">GTT-Net: Learned Generalized Trajectory Triangulation"</span> by X. Xu and E. Dunn, was accepted for oral presentation in ICCV 2021. <strong>Acceptance rate 3%</strong>.
        </div>
      </div>

    </div>
</div>


<!-- Post VOLDOR+SLAM -->
  <div class="col-md-12 wow fadeIn pb-30" >

    <div class="row">
      <div class="col-md-4 blog2-post-title-cont">
        <div class="post-prev-date-cont">
          <span class="blog2-date-numb">    2021</span><span class="blog2-month">MAY</span>
        </div>
        <div class="post-prev-title">
          <h3><a href="">VOLDOR+SLAM Code Release</a></h3>
          <div class="post-prev-info">

          </div>
        </div>
      </div>

      <div class="col-md-8">
        <div class="blog2-post-prev-text">
          The open-source release of our visual odomettry and SLAM pipeline is now available in
          <a href="https://github.com/htkseason/VOLDOR">github</a>.
          This release consitutes the code base for our recently accepted ICRA-21 paper "
          <span class="bold">VOLDOR+SLAM: For the times when feature-based or direct methods are not good enough"</span>
          by  Z. Min and  E. Dunn, and  our award-winning submissions (for both monocular and stereo tracks) to the
          <a href="https://sites.google.com/view/vislocslamcvpr2020/slam-challenge">CVPR'20 SLAM Challenge</a>.
        </div>
      </div>

    </div>
</div>

<!-- Post Item CVPR ORAL -->
  <div class="col-md-12 wow fadeIn pb-30" >

    <div class="row">
      <div class="col-md-4 blog2-post-title-cont">
        <div class="post-prev-date-cont">

          <span class="blog2-date-numb">2020</span><span class="blog2-month">MAY</span>
        </div>
        <div class="post-prev-title">
          <h3><a href="">CVPR 2020 Oral</a></h3>
          <div class="post-prev-info">

          </div>
        </div>
      </div>

      <div class="col-md-8">
        <div class="blog2-post-prev-text">
          Our paper "<span class="bold">VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals"</span> by  Z. Min, Y. Yang and E. Dunn, was accepted for oral presentation in CVPR 2020. <strong>Acceptance rate 5.7%</strong>.
        </div>
      </div>

    </div>
</div>
              <!-- Post Item ICCV ORAL -->
                <div class="col-md-12 wow fadeIn pb-30" >

                  <div class="row">
                    <div class="col-md-4 blog2-post-title-cont">
                      <div class="post-prev-date-cont">
                        <span class="blog2-date-numb">    2019</span><span class="blog2-month">JULY</span>
                      </div>
                      <div class="post-prev-title">
                        <h3><a href="">ICCV 2019 Oral</a></h3>
                        <div class="post-prev-info">

                        </div>
                      </div>
                    </div>

                    <div class="col-md-8">
                      <div class="blog2-post-prev-text">
                        Our paper "<span class="bold">Discrete Laplace operator estimation for dynamic 3D reconstruction"</span> by X. Xu and E. Dunn, was accepted for oral presentation in ICCV 2019. <strong>Acceptance rate 4.3%</strong>.
                      </div>
                    </div>

                  </div>
              </div>
            <!-- Post Item Tutorial CVPR16 -->
              <div class="col-md-12 wow fadeIn pb-30" >

                <div class="row">
                  <div class="col-md-4 blog2-post-title-cont">
                    <div class="post-prev-date-cont">
                      <span class="blog2-date-numb">    2017</span><span class="blog2-month">JULY</span>
                    </div>
                    <div class="post-prev-title">
                      <h3><a href="">CVPR 2017 Tutorial</a></h3>
                      <div class="post-prev-info">

                      </div>
                    </div>
                  </div>

                  <div class="col-md-8">
                    <div class="blog2-post-prev-text">
                      The day-long tutorial "<span class="bold">Large-Scale 3D Modeling from Crowdsourced Data"</span> was presented in CVPR 2017 by	J. M. Frahm, E. Dunn, M. Pollefeys, J. Heinly and J. L. Schönberger
                        [<a  href="https://demuc.de/tutorials/cvpr2017/">WebSite</a>]
                    </div>
                  </div>

                </div>
            </div>


              <!-- Post Item PAMI -->
              <div class="col-md-12 wow fadeIn pb-30" >
                <div class="row">

                  <div class="col-md-4 blog2-post-title-cont">
                    <div class="post-prev-date-cont">
                      <span class="blog2-date-numb">2017</span><span class="blog2-month">JUNE</span>
                    </div>
                    <div class="post-prev-title">
                      <h3><a href="">TPAMI Paper Accepted</a></h3>
                      <div class="post-prev-info">

                      </div>
                    </div>
                  </div>

                  <div class="col-md-8">
                    <div class="blog2-post-prev-text">
                      Our paper "<span class="bold">Self-expressive Dictionary Learning for Dynamic 3D Reconstruction</span>" by E. Zheng, D. Ji, E. Dunn and J. M. Frahm,  was accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence. This work is an extension of our ICCV 2015 paper.
                        [ <a href="https://arxiv.org/pdf/1605.06863.pdf">Arxiv</a><span class="slash-divider">/</span>
                          <a href="http://ieeexplore.ieee.org/document/8014489/">IEEE</a> ]
                    </div>
                  </div>

                </div>
              </div>
            <!-- Post Item ISMAR -->
              <div class="col-md-12 wow fadeIn pb-30" >
                <div class="row">

                  <div class="col-md-4 blog2-post-title-cont">
                    <div class="post-prev-date-cont">
                      <span class="blog2-date-numb">2016</span><span class="blog2-month">SEPT</span>
                    </div>
                    <div class="post-prev-title">
                      <h3><a href="">ISMAR BEST PAPER</a></h3>
                      <div class="post-prev-info">

                      </div>
                    </div>
                  </div>

                  <div class="col-md-8">
                    <div class="blog2-post-prev-text">
                     Our paper  "<span class="bold">Towards kilo-hertz 6-DoF visual tracking using an egocentric cluster of rolling shutter cameras</span>" by A. Bapat, E. Dunn and J. M. Frahm, received the best paper award in the recent <a href="http://www.ismar2016.vgtc.org/"> ISMAR 2016  </a> held at Merida, Mexico. Moreover, an extended version of the paper has been accepted into a special issue of  IEEE Transactions on Visualization and Computer Graphics
                        [ <a href="pubs/TVCG16_Bapat_etal.pdf">Pre-Print</a><span class="slash-divider">/</span>
                          <a href="http://ieeexplore.ieee.org/document/7523411/">IEEE </a> ]
                    </div>
                  </div>

                </div>
              </div>





           </div>






            <div id="research-link" class="mb-10">
              <h2 class="section-title pr-0">RESEARCH <span class="bold">PROJECTS</span>
                  </h2>
            </div>
        <div class=" plr-10 plr-0-767 clearfix">
        <!-- COTENT CONTAINER -->
        <div class=" plr-10 pt-20 pb-20">

          <div class="relative">
            <!-- PORTFOLIO FILTER -->
            <div class="port-filter text-center text-left-767">
              <a href="#" class="filter active" data-filter="*">All </a>
              <a href="#" class="filter" data-filter=".3D">3D Reconstruction</a>
              <a href="#" class="filter" data-filter=".Dynamics"> Visual Dynamics</a>
              <a href="#" class="filter" data-filter=".Retrieval">Content Retrieval</a>
<!--              <a href="#" class="filter" data-filter=".design">Augmented Reality</a>-->

            </div>

            <!-- ITEMS GRID -->
            <ul class="port-grid port-grid-3 port-grid-gut clearfix" id="items-grid">

              <!-- Item 1 -->

              <li class="port-item mix 3D">
                <a href="portfolio-single1.html">
                  <div class="port-img-overlay">
                    <img class="port-main-img"
                                         src="images/dunn2_dup.jpg" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a>DUPLICATE STRUCTURE CORRECTION IN SFM</a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">
                      <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em" data-toggle="modal" data-target=".modal-conf_eccv_HeinlyDF14-abs"></div></a>
                      <a href="https://www.youtube.com/watch?time_continue=80&v=ucmC0-HJIkg"  class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>
                </div>
              </li>
<!-- Item 2 -->
              <li class="port-item mix 3D">
                <a href="portfolio-single1.html">
                  <div class="port-img-overlay">
                    <img class="port-main-img"
                                         src="images/dunn2_reconstructing_the_world_640.jpg" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a>RECONSTRUCTING THE WORLD IN SIX DAYS</a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">

                      <a  class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em" data-toggle="modal" data-target=".modal-conf_cvpr_HeinlySDF15-abs"></div></a>

                      <a href="https://www.youtube.com/watch?time_continue=1&v=bRYqyoqUJuM"  class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>

                </div>
              </li>
<!-- Item 3 -->
              <li class="port-item mix Retrieval">
                <a href="portfolio-single1.html">
                  <div class="port-img-overlay">
                    <img class="port-main-img"  width="100"
                                         src="images/dunn2_crn.png" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a>LEARNED CONTEXTUAL FEATURE REWEIGHTING </a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">

                        <a class="mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em"  data-toggle="modal" data-target=".modal-CVPR17_Kim_etal-abs"></div></a>

                      <a href="https://www.youtube.com/watch?v=BoN2fWDgsyU"
                         class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>

                </div>
              </li>



              <!-- Item 4 -->
              <li class="port-item mix Retrieval">
                <a>
                  <div class="port-img-overlay">
                    <img class="port-main-img"  width="100"
                                         src="images/dunn2_pbvlad.png" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a>FEATURE SELECTION USING PER-BUNDLE VLAD</a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">
                      <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em" data-toggle="modal" data-target=".modal-conf_iccv_KimDF15-abs"></div></a>

                      <a href="https://www.youtube.com/watch?v=X6FxD6zsZQc"
                         class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>

                </div>
              </li>

<!-- Item 5 -->
              <li class="port-item mix 3D">
                <a>
                  <div class="port-img-overlay">
                    <img class="port-main-img"  width="100"
                                         src="images/dunn2_mvs.png" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a>JOINT VIEW SELECTION AND DEPTMAP ESTIMATION</a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">
                      <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em" data-toggle="modal" data-target=".modal-conf_cvpr_ZhengDJF14-abs"></div></a>

                      <a href="https://www.youtube.com/watch?v=vxPZxdDYXYw"
                         class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>

                </div>
              </li>


<!-- Item 6 -->
                <li class="port-item mix Dynamics">
                <a>
                  <div class="port-img-overlay">
                    <img class="port-main-img"  width="100"
                                         src="images/dunn2_mosaic_iccv_2015_jdh.png" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a> SYNTHETIC ILLUMINATION MOSAICS </a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">
                      <a  class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em"  data-toggle="modal" data-target=".modal-conf_iccv_JiDF15-abs"></div></a>
                      <a href="images/DV_MOSAIC_iccv2015.mp4" class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>
                </div>
              </li>

<!-- Item 7 -->
                <li class="port-item mix Dynamics">
                <a>
                  <div class="port-img-overlay">
                    <img class="port-main-img"  width="100"
                                         src="images/dunn2_4dmatch.png" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a> DENSE SPATIO-TEMPORAL  CORRESPONDENCE  </a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">
                      <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em" data-toggle="modal" data-target=".modal-conf_eccv_JiDF16-abs"></div></a>
                      <a href="images/DV_4DMATCHING_eccv2016.mp4"  class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>
                </div>
              </li>

              <!-- Item 8 -->
                <li class="port-item mix Dynamics">
                <a>
                  <div class="port-img-overlay">
                    <img class="port-main-img"  width="100"
                                         src="images/dunn2_dt.png" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a> 3D RECONSTRUCTION OF DYNAMIC TEXTURES</a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">
                      <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em" data-toggle="modal" data-target=".modal-conf_eccv_JiDF14-abs"></div></a>
                      <a href="https://www.youtube.com/watch?v=B2xldZPi_i0" class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>
                </div>
              </li>


<!-- Item 9 -->
              <li class="port-item mix 3D">
                <a>
                  <div class="port-img-overlay">
                    <img class="port-main-img"  width="100"
                                         src="images/dunn2_bmt.png" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a>BRINGING 3D MODELS TOGETHER</a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">
                      <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em" data-toggle="modal" data-target=".modal-conf_accv_WangDRF16-abs"></div></a>
                      <a href="https://www.youtube.com/watch?v=nNg7Z3Pf81k"
                         class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>

                </div>
              </li>
<!-- Item 10 -->
              <li class="port-item mix Dynamics">
                <a>
                  <div class="port-img-overlay">
                    <img class="port-main-img"  width="100"
                                         src="images/dunn2_jost.png" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a>OBJECT CLASS SEQUENCING AND TRAJECTORY TRIANGULATION</a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">
                      <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em" data-toggle="modal" data-target=".modal-conf_eccv_ZhengWDF14-abs"></div></a>
                      <a href="https://www.youtube.com/watch?v=xMbn4-u5hdA"
                         class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>

                </div>
              </li>
 <!-- Item 11 -->
              <li class="port-item mix 3D">
                <a>
                  <div class="port-img-overlay">
                    <img class="port-main-img"  width="100"
                                         src="images/dunn2_sos3.png" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a>STEREO UNDER SEQUENTIAL OPTIMAL SAMPLING</a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">
                      <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em"  data-toggle="modal" data-target=".modal-conf_cvpr_WangWDF14-abs"></div></a>
                      <a href="https://www.youtube.com/watch?v=bNk6xOIH_Cw"
                         class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>

                </div>
              </li>

    <!-- Item 12 -->
             <li class="port-item mix 3D">
               <a>
                 <div class="port-img-overlay">
                   <img class="port-main-img"  width="100"
                                        src="images/voldor1.png" alt="img" >
                 </div>
               </a>
               <div class="port-overlay-cont">
                   <div class="port-title-cont" style="text-align:center">
                                         <h3><a>VOLDOR: <br>DENSE INDIRECT VISUAL ODOMETRY </a></h3>
                   </div>
                   <div class="port-btn-cont" style="text-align:center">
                     <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em"  data-toggle="modal" data-target=".modal-conf_cvpr_WangWDF14-abs"></div></a>
                     <a href="https://www.youtube.com/watch?v=wlWjSTiyE4s"
                        class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                   </div>

               </div>
             </li>

     <!-- Item 12 -->
              <li class="port-item mix 3D">
                <a>
                  <div class="port-img-overlay">
                    <img class="port-main-img"  width="100"
                                         src="images/voldor2.png" alt="img" >
                  </div>
                </a>
                <div class="port-overlay-cont">
                    <div class="port-title-cont" style="text-align:center">
                                          <h3><a>VOLDOR+SLAM:</a></h3>
                    </div>
                    <div class="port-btn-cont" style="text-align:center">
                      <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em"  data-toggle="modal" data-target=".modal-conf_cvpr_WangWDF14-abs"></div></a>
                      <a href="https://www.youtube.com/watch?v=G3ornaZO1Mc"
                         class="popup-youtube"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                    </div>

                </div>
              </li>
    <!-- Item 13 -->
             <li class="port-item mix Dynamics">
               <a>
                 <div class="port-img-overlay">
                   <img class="port-main-img"  width="100"
                                        src="images/DLOE.png" alt="img" >
                 </div>
               </a>
               <div class="port-overlay-cont">
                   <div class="port-title-cont" style="text-align:center">
                                         <h3><a>DISCRETE LAPLACE OPERATOR ESTIMATION FOR DYNAMIC 3D RECONSTRUCTION</a></h3>
                   </div>
                   <div class="port-btn-cont" style="text-align:center">
                     <a class=" mr-20" ><div aria-hidden="true" class="icon-basic-info" style="font-size: 2em"  data-toggle="modal" data-target=".modal-conf_cvpr_WangWDF14-abs"></div></a>
                     <a href=" https://youtu.be/my3jocjpD0U?t=2290s"
                        class="popup-youtubex"><div aria-hidden="true" class="icon icon-music-play-button" style="font-size: 2em"></div></a>
                   </div>

               </div>
             </li>



            </ul>

          </div>

        </div>

      </div>


<!--           </div>   -->

            <div id="papers-link" class="mb-50">
              <h2 class="section-title pr-0">ACADEMIC <span class="bold">PUBLICATIONS</span>
                  </h2>
            </div>
            <div class="mb-50">
                <dl class="dl-horizontal">


<!--     mystart  references               -->
<dt>  <a href="pubs/ICRA21-Min.pdf" class="icon icon-basic-download" ></a>
      <a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-ICRA21-Min-abs"></a>
      <a class="icon-basic-info" data-toggle="modal" data-target=".modal-ICRA21-Min-ref"></a>
</dt>
<dd>  <span class="bold"> VOLDOR-SLAM: For the Times When Feature-Based or Direct Methods Are Not Good Enough</span> <br>
      <i> Zhixiang Min and Enrique Dunn </i><br>
      International Conference on Robotics and Automation (ICRA 2021)
</dd>
<div class="modal fade modal-ICRA21-Min-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
      <div class="modal-dialog modal-lg">
        <div class="modal-body">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
              <h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
            </div>
            <div class="modal-body">
              @article{DBLP:journals/corr/abs-2104-06800,
                author    = {Zhixiang Min and
                             Enrique Dunn},
                title     = {VOLDOR+SLAM: For the Times When Feature-Based or Direct Methods
                             Are Not Good Enough},
                journal   = {CoRR},
                volume    = {abs/2104.06800},
                year      = {2021},
                url       = {https://arxiv.org/abs/2104.06800},
                archivePrefix = {arXiv},
                eprint    = {2104.06800}
              }
            </div>
            </div>
          </div>
        </div>
</div>
<div class="modal fade modal-ICRA21-Min-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-lg">
    <div class="modal-body">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
          <h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
        </div>
        <div class="modal-body">
          We present a dense-indirect SLAM system using external dense optical flows as input. We extend the recent probabilistic visual odometry model VOLDOR [Min et al. CVPR'20], by incorporating the use of geometric priors to 1) robustly bootstrap estimation from monocular capture, while 2) seamlessly supporting stereo and/or RGB-D input imagery. Our customized back-end tightly couples our intermediate geometric estimates with an adaptive priority scheme managing the connectivity of an incremental pose graph. We leverage recent advances in dense optical flow methods to achieve accurate and robust camera pose estimates, while constructing fine-grain globally-consistent dense environmental maps. Our open source implementation [this https URL] operates online at around 15 FPS on a single GTX1080Ti GPU.      </div>
        </div>
    </div>
  </div>
</div>


<dt>  <a href="pubs/CVPR20-Min.pdf" class="icon icon-basic-download" ></a>
      <a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-CVPR20-Min-abs"></a>
      <a class="icon-basic-info" data-toggle="modal" data-target=".modal-CVPR20-Min-ref"></a>
</dt>
<dd>  <span class="bold"> VOLDOR: Visual Odometry From Log-Logistic Dense Optical Flow Residuals</span> <br>
      <i> Zhixiang Min, Yiding Yang and Enrique Dunn </i><br>
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020)
</dd>
<div class="modal fade modal-CVPR20-Min-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
      <div class="modal-dialog modal-lg">
        <div class="modal-body">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
              <h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
            </div>
            <div class="modal-body">
              @inproceedings{cvprMinYD20,
                    author    = {Zhixiang Min and
                                 Yiding Yang and
                                 Enrique Dunn},
                    title     = {{VOLDOR:} Visual Odometry From Log-Logistic Dense Optical Flow Residuals},
                    booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
                                 {CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
                    pages     = {4897--4908},
                    publisher = {{IEEE}},
                    year      = {2020},
                    url       = {https://doi.org/10.1109/CVPR42600.2020.00495},
                    doi       = {10.1109/CVPR42600.2020.00495},
                  }
            </div>
            </div>
          </div>
        </div>
</div>
<div class="modal fade modal-CVPR20-Min-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-lg">
    <div class="modal-body">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
          <h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
        </div>
        <div class="modal-body">
          We propose a dense indirect visual odometry method taking as input externally estimated optical flow fields instead of hand-crafted feature correspondences. We define our problem as a probabilistic model and develop a generalized-EM formulation for the joint inference of camera motion, pixel depth, and motion-track confidence. Contrary to traditional methods assuming Gaussian-distributed observation errors, we supervise our inference framework under an (empirically validated) adaptive log-logistic distribution model. Moreover, the log-logistic residual model generalizes well to different state-of-the-art optical flow methods, making our approach modular and agnostic to the choice of optical flow estimators. Our method achieved top-ranking results on both TUM RGB-D and KITTI odometry benchmarks. Our open-sourced implementation is inherently GPU-friendly with only linear computational and storage growth.        </div>
      </div>
    </div>
  </div>
</div>



<dt>  <a href="pubs/ICCV19-XuDunn.pdf" class="icon icon-basic-download" ></a>
      <a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-ICCV19-XuDunn-abs"></a>
      <a class="icon-basic-info" data-toggle="modal" data-target=".modal-ICCV19-XuDunn-ref"></a>
</dt>
<dd>  <span class="bold"> Discrete Laplace Operator Estimation for Dynamic 3D Reconstruction</span> <br>
      <i> Xiangyu Xu and  Enrique Dunn </i><br>
      IEEE International Conference on Computer Vision (ICCV 2019)
</dd>
<div class="modal fade modal-ICCV19-XuDunn-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
      <div class="modal-dialog modal-lg">
        <div class="modal-body">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
              <h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
            </div>
            <div class="modal-body">
              @inproceedings{XuICCV19,  author    = {Xiangyu Xu and Enrique Dunn},title     = {Discrete Laplace Operator Estimation for Dynamic 3D Reconstruction}, booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV} 2019, Seoul, Korea (South), October 27 - November 2, 2019},  pages     = {1548--1557},  publisher = {{IEEE}},  year      = {2019},   doi = {10.1109/ICCV.2019.00163}}
            </div>
            </div>
          </div>
        </div>
</div>
<div class="modal fade modal-ICCV19-XuDunn-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-lg">
    <div class="modal-body">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
          <h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
        </div>
        <div class="modal-body">
          We present a general paradigm for dynamic 3D reconstruction from multiple independent and uncontrolled image sources having arbitrary temporal sampling density and distribution. Our graph-theoretic formulation models the spatio-temporal relationships among our observations in terms of the joint estimation of their 3D geometry and its discrete Laplace operator. Towards this end, we define a tri-convex optimization framework that leverages the geometric properties and dependencies found among a Euclidean shape-space and the discrete Laplace operator describing its local and global topology. We present a reconstructability analysis, experiments on motion capture data and multi-view image datasets, as well as explore applications to geometrybased event segmentation and data association.
        </div>
      </div>
    </div>
  </div>
</div>



<dt>  <a href="pubs/PAMI17_Zheng_etal.pdf" class="icon icon-basic-download" ></a>
      <a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-PAMI17_Zheng_etal-abs"></a>
      <a class="icon-basic-info" data-toggle="modal" data-target=".modal-PAMI17_Zheng_etal-ref"></a>
</dt>
<dd>  <span class="bold"> Self-expressive Dictionary Learning for Dynamic 3D Reconstruction</span> <br>
      <i> Enliang Zheng, Dinghuang Ji, Enrique Dunn, Jan-Michael Frahm </i><br>
      IEEE Transactions on Pattern Analysis and Machine Intelligence
</dd>
<div class="modal fade modal-PAMI17_Zheng_etal-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
      <div class="modal-dialog modal-lg">
        <div class="modal-body">
          <div class="modal-content">
            <div class="modal-header">
              <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
              <h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
            </div>
            <div class="modal-body">
              @article{zheng2017self, title={Self-expressive Dictionary Learning for Dynamic 3D Reconstruction}, author={Enliang Zheng and Dinghuang Ji and Enrique Dunn and Jan-Michael Frahm},journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},year={2017},publisher=IEEE}
</div>
            </div>
          </div>
        </div>
</div>

<div class="modal fade modal-PAMI17_Zheng_etal-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-lg">
    <div class="modal-body">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
          <h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
        </div>
        <div class="modal-body">
          We target the problem of sparse 3D reconstruction of dynamic objects observed by multiple unsynchronized video cameras with unknown temporal overlap. To this end, we develop a framework to recover the unknown structure without sequencing information across video sequences. Our proposed compressed sensing framework poses the estimation of 3D structure as the problem of dictionary learning, where the dictionary is defined as an aggregation of the temporally varying 3D structures. Given the smooth motion of dynamic objects, we observe any element in the dictionary can be well approximated by a sparse linear combination of other elements in the same dictionary (i.e. self-expression). Our formulation optimizes a biconvex cost function that leverages a compressed sensing formulation and enforces both structural dependency coherence across video streams, as well as motion smoothness across estimates from common video sources. We further analyze the reconstructability of our approach under different capture scenarios, and its comparison and relation to existing methods. Experimental results on large amounts of synthetic data as well as real imagery demonstrate the effectiveness of our approach.
        </div>
      </div>
    </div>
  </div>
</div>

<dt> <a href="pubs/CVPR17_Kim_etal.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-CVPR17_Kim_etal-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-CVPR17_Kim_etal-ref"></a>
</dt>
<dd> <span class="bold"> Learned Contextual Feature Reweighting for Image Geo-Localization</span> <br> <i> Hyo-Jin Kim, Enrique Dunn, Jan-Michael Frahm </i><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)</dd>
<div class="modal fade modal-CVPR17_Kim_etal-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{kim2017learned, title={Learned Contextual Feature Reweighting for Image Geo-Localization}, author= {Hyo-Jin Kim and Enrique Dunn and Jan-Michael Frahm}, booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, year={2017} }
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-CVPR17_Kim_etal-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We address the problem of large scale image geo- localization where the location of an image is estimated by identifying geo-tagged reference images depicting the same place. We propose a novel model for learning image rep- resentations that integrates context-aware feature reweight- ing in order to effectively focus on regions that positively contribute to geo-localization. In particular, we introduce a Contextual Reweighting Network (CRN) that predicts the importance of each region in the feature map based on the image context. Our model is learned end-to-end for the im- age geo-localization task, and requires no annotation other than image geo-tags for training. In experimental results, the proposed approach significantly outperforms the previ- ous state-of-the-art on the standard geo-localization bench- mark datasets.We also demonstrate that our CRN discovers task-relevant contexts without any additional supervision.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/TVCG16_Bapat_etal.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-TVCG16_Bapat_etal-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-TVCG16_Bapat_etal-ref"></a>
</dt>
<dd> <span class="bold"> Towards Kilo-Hertz 6-DoF Visual Tracking Using an Egocentric Cluster
of Rolling Shutter Cameras</span> <br> <i> Akash Bapat,
Enrique Dunn,
Jan-Michael Frahm </i><br> IEEE Transactions on Visualization and Computer Graphics</dd>
<div class="modal fade modal-TVCG16_Bapat_etal-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@article{DBLP:journals/tvcg/BapatDF16,
author = {Akash Bapat and
Enrique Dunn and
Jan-Michael Frahm},
title = {Towards Kilo-Hertz 6-DoF Visual Tracking Using an Egocentric Cluster
of Rolling Shutter Cameras},
journal = {IEEE Trans. Vis. Comput. Graph.},
volume = {22},
number = {11},
pages = {2358--2367},
year = {2016}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-TVCG16_Bapat_etal-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
To maintain a reliable registration of the virtual world with the real world, augmented reality (AR) applications require highly accurate, low-latency tracking of the device. In this paper, we propose a novel method for performing this fast 6-DOF head pose tracking using a cluster of rolling shutter cameras. The key idea is that a rolling shutter camera works by capturing the rows of an image in rapid succession, essentially acting as a high-frequency 1D image sensor. By integrating multiple rolling shutter cameras on the AR device, our tracker is able to perform 6-DOF markerless tracking in a static indoor environment with minimal latency. Compared to state-of-the-art tracking systems, this tracking approach performs at significantly higher frequency, and it works in generalized environments. To demonstrate the feasibility of our system, we present thorough evaluations on synthetically generated data with tracking frequencies reaching 56.7 kHz. We further validate the method's accuracy on real-world images collected from a prototype of our tracking system against ground truth data using standard commodity GoPro cameras capturing at 120 Hz frame rate.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_accv_WangDRF16.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_accv_WangDRF16-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_accv_WangDRF16-ref"></a>
</dt>
<dd> <span class="bold"> Bringing 3D Models Together: Mining Video Liaisons in Crowdsourced
Reconstructions</span> <br> <i> Ke Wang,
Enrique Dunn,
Mikel Rodriguez,
Jan-Michael Frahm </i><br> Asian Conference on Computer Vision (ACCV 2016)</dd>
<div class="modal fade modal-conf_accv_WangDRF16-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/accv/WangDRF16,
author = {Ke Wang and
Enrique Dunn and
Mikel Rodriguez and
Jan-Michael Frahm},
title = {Bringing 3D Models Together: Mining Video Liaisons in Crowdsourced
Reconstructions},
booktitle = {Computer Vision - {ACCV} 2016 - 13th Asian Conference on Computer
Vision, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers,
Part {IV}},
pages = {408--423},
year = {2016}}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_accv_WangDRF16-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
The recent advances in large-scale scene modeling have enabled the automatic 3D reconstruction of landmark sites from crowdsourced photo collections. Here, we address the challenge of leveraging crowdsourced video collections to identify connecting visual observations that enable the alignment and subsequent aggregation, of disjoint 3D models. We denote these connecting image sequences as video liaisons and develop a data-driven framework for fully unsupervised extraction and exploitation. Towards this end, we represent video contents in terms of a histogram representation of iconic imagery contained within existing 3D models attained from a photo collection. We then use this representation to efficiently identify and prioritize the analysis of individual videos within a large-scale video collection, in an effort to determine camera motion trajectories connecting different landmarks. Results on crowdsourced data illustrate the efficiency and effectiveness of our proposed approach.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_eccv_JiDF16.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_eccv_JiDF16-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_eccv_JiDF16-ref"></a>
</dt>
<dd> <span class="bold"> Spatio-Temporally Consistent Correspondence for Dense Dynamic Scene
Modeling</span> <br> <i> Dinghuang Ji,
Enrique Dunn,
Jan-Michael Frahm </i><br>
European Conference on Computer Vision (ECCV 2016) </dd>
<div class="modal fade modal-conf_eccv_JiDF16-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/eccv/JiDF16,
author = {Dinghuang Ji and
Enrique Dunn and
Jan-Michael Frahm},
title = {Spatio-Temporally Consistent Correspondence for Dense Dynamic Scene
Modeling},
booktitle = {Computer Vision - {ECCV} 2016 - 14th European Conference, Amsterdam,
The Netherlands, October 11-14, 2016, Proceedings, Part {VI}},
pages = {3--18},
year = {2016}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_eccv_JiDF16-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We address the problem of robust two-view correspondence estimation within the context of dynamic scene modeling. To this end, we investigate the use of local spatio-temporal assumptions to both identify and refine dense low-level data associations in the absence of prior dynamic content models. By developing a strictly data-driven approach to correspondence search, based on bottom-up local 3D motion cues of local rigidity and non-local coherence, we are able to robustly address the higher-order problems of video synchronization and dynamic surface modeling. Our findings suggest an important relationship between these two tasks, in that maximizing spatial coherence of surface points serves as a direct metric for the temporal alignment of local image sequences. The obtained results for these two problems on multiple publicly available dynamic reconstruction datasets illustrate both the effectiveness and generality of our proposed approach.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_wacv_WangSDF16.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_wacv_WangSDF16-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_wacv_WangSDF16-ref"></a>
</dt>
<dd> <span class="bold"> Efficient joint stereo estimation and land usage classification for
multiview satellite data</span> <br> <i> Ke Wang,
Craig Stutts,
Enrique Dunn,
Jan-Michael Frahm </i><br>
IEEE Winter Conference on Applications of Computer Vision (WACV 2016)</dd>
<div class="modal fade modal-conf_wacv_WangSDF16-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/wacv/WangSDF16,
author = {Ke Wang and
Craig Stutts and
Enrique Dunn and
Jan-Michael Frahm},
title = {Efficient joint stereo estimation and land usage classification for
multiview satellite data},
booktitle = {2016 IEEE Winter Conference on Applications of Computer Vision,
{WACV} 2016, Lake Placid, NY, USA, March 7-10, 2016},
pages = {1--9},
year = {2016}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_wacv_WangSDF16-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We propose an efficient algorithm to jointly estimate geometry and semantics for a given geographical region observed by multiple satellite images. Our joint estimation leverages an efficient PatchMatch inference framework de- fined over lattice discretization of the environment. Our cost function relies on the local planarity assumption to model scene geometry and neural network classification to determine semantic (e.g. land use) labels for geometric structures. By utilizing the commonly available direct (i.e. space to image) rational polynomial coefficients (RPC) satellite camera models, our approach effectively circumvents the need for estimating or refining inverse RPC models. Experiments illustrate both the computational efficiency and high quality scene geometry estimates attained by our approach for satellite imagery. To further illustrate the generality of our representation and inference framework, experiments on standard benchmarks for ground-level imagery are also included.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_cvpr_HeinlySDF15.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_cvpr_HeinlySDF15-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_cvpr_HeinlySDF15-ref"></a>
</dt>
<dd> <span class="bold"> Reconstructing the world* in six days *(As Captured by the Yahoo 100 Million Image Dataset)</span> <br> <i> Jared Heinly,
Johannes L. Schonberger,
Enrique Dunn,
Jan-Michael Frahm </i><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)</dd>
<div class="modal fade modal-conf_cvpr_HeinlySDF15-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/cvpr/HeinlySDF15,
author = {Jared Heinly and
Johannes L. Schonberger and
Enrique Dunn and
Jan-Michael Frahm},
title = {Reconstructing the world* in six days *(As Captured by the Yahoo 100 Million Image Dataset)},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition, {CVPR}
2015, Boston, MA, USA, June 7-12, 2015},
pages = {3287--3295},
year = {2015}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_cvpr_HeinlySDF15-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We propose a novel, large-scale, structure-from-motion framework that advances the state of the art in data scalability from city-scale modeling (millions of images) to world-scale modeling (several tens of millions of images) using just a single computer. The main enabling technology is the use of a streaming-based framework for connected component discovery. Moreover, our system employs an adaptive, online, iconic image clustering approach based on an augmented bag-of-words representation, in order to balance the goals of registration, comprehensiveness, and data compactness. We demonstrate our proposal by operating on a recent publicly available 100 million image crowdsourced photo collection containing images geographically distributed throughout the entire world. Results illustrate that our streaming-based approach does not compromise model completeness, but achieves unprecedented levels of efficiency and scalability.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_iccv_JiDF15.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_iccv_JiDF15-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_iccv_JiDF15-ref"></a>
</dt>
<dd> <span class="bold"> Synthesizing Illumination Mosaics from Internet Photo-Collections</span> <br> <i> Dinghuang Ji,
Enrique Dunn,
Jan-Michael Frahm </i><br>
IEEE International Conference on Computer Vision (ICCV 2015)</dd>
<div class="modal fade modal-conf_iccv_JiDF15-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/iccv/JiDF15,
author = {Dinghuang Ji and
Enrique Dunn and
Jan-Michael Frahm},
title = {Synthesizing Illumination Mosaics from Internet Photo-Collections},
booktitle = {2015 IEEE International Conference on Computer Vision, {ICCV} 2015,
Santiago, Chile, December 7-13, 2015},
pages = {3988--3996},
year = {2015}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_iccv_JiDF15-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We propose a framework for the automatic creation of time-lapse mosaics of a given scene. We achieve this by leveraging the illumination variations captured in Internet photo-collections. In order to depict and characterize the illumination spectrum of a scene, our method relies on building discrete representations of the image appearance space through connectivity graphs defined over a pairwise image distance function. The smooth appearance transitions are found as the shortest path in the similarity graph among images, and robust image alignment is achieved by leveraging scene semantics, multi-view geometry, and image warping techniques. The attained results present an insightful and compact visualization of the scene illuminations captured in crowd-sourced imagery.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_iccv_KimDF15.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_iccv_KimDF15-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_iccv_KimDF15-ref"></a>
</dt>
<dd> <span class="bold"> Predicting Good Features for Image Geo-Localization Using Per-Bundle
VLAD</span> <br> <i> Hyo Jin Kim,
Enrique Dunn,
Jan-Michael Frahm </i><br>
IEEE International Conference on Computer Vision (ICCV 2015)</dd>
<div class="modal fade modal-conf_iccv_KimDF15-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/iccv/KimDF15,
author = {Hyo Jin Kim and
Enrique Dunn and
Jan-Michael Frahm},
title = {Predicting Good Features for Image Geo-Localization Using Per-Bundle
VLAD},
booktitle = {2015 IEEE International Conference on Computer Vision, {ICCV} 2015,
Santiago, Chile, December 7-13, 2015},
pages = {1170--1178},
year = {2015}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_iccv_KimDF15-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We address the problem of recognizing a place depicted in a query image by using a large database of geo-tagged images at a city-scale. In particular, we discover features that are useful for recognizing a place in a data-driven manner, and use this knowledge to predict useful features in a query image prior to the geo-localization process. This allows us to achieve better performance while reducing the number of features. Also, for both learning to predict features and retrieving geo-tagged images from the database, we propose per-bundle vector of locally aggregated descriptors (PBVLAD), where each maximally stable region is described by a vector of locally aggregated descriptors (VLAD) on multiple scale-invariant features detected within the region. Experimental results show the proposed approach achieves a significant improvement over other baseline methods.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_iccv_ZhengJDF15.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_iccv_ZhengJDF15-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_iccv_ZhengJDF15-ref"></a>
</dt>
<dd> <span class="bold"> Sparse Dynamic 3D Reconstruction from Unsynchronized Videos</span> <br> <i> Enliang Zheng,
Dinghuang Ji,
Enrique Dunn,
Jan-Michael Frahm </i><br> IEEE International Conference on Computer Vision (ICCV 2015)</dd>
<div class="modal fade modal-conf_iccv_ZhengJDF15-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/iccv/ZhengJDF15,
author = {Enliang Zheng and
Dinghuang Ji and
Enrique Dunn and
Jan-Michael Frahm},
title = {Sparse Dynamic 3D Reconstruction from Unsynchronized Videos},
booktitle = {2015 IEEE International Conference on Computer Vision, {ICCV} 2015,
Santiago, Chile, December 7-13, 2015},
pages = {4435--4443},
year = {2015}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_iccv_ZhengJDF15-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We target the sparse 3D reconstruction of dynamic objects observed by multiple unsynchronized video cameras with unknown temporal overlap. To this end, we develop a framework to recover the unknown structure without sequencing information across video sequences. Our proposed compressed sensing framework poses the estimation of 3D structure as the problem of dictionary learning. Moreover, we define our dictionary as the temporally varying 3D structure, while we define local sequencing information in terms of the sparse coefficients describing a locally linear 3D structural interpolation. Our formulation optimizes a biconvex cost function that leverages a compressed sensing formulation and enforces both structural dependency coherence across video streams, as well as motion smoothness across estimates from common video sources. Experimental results demonstrate the effectiveness of our approach in both synthetic data and captured imagery.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_iccv_ZhengWDF15.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_iccv_ZhengWDF15-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_iccv_ZhengWDF15-ref"></a>
</dt>
<dd> <span class="bold"> Minimal Solvers for 3D Geometry from Satellite Imagery</span> <br> <i> Enliang Zheng,
Ke Wang,
Enrique Dunn,
Jan-Michael Frahm </i><br>
IEEE International Conference on Computer Vision (ICCV 2015)</dd>
<div class="modal fade modal-conf_iccv_ZhengWDF15-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/iccv/ZhengWDF15,
author = {Enliang Zheng and
Ke Wang and
Enrique Dunn and
Jan-Michael Frahm},
title = {Minimal Solvers for 3D Geometry from Satellite Imagery},
booktitle = {2015 IEEE International Conference on Computer Vision, {ICCV} 2015,
Santiago, Chile, December 7-13, 2015},
pages = {738--746},
year = {2015}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_iccv_ZhengWDF15-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We propose two novel minimal solvers which advance the state of the art in satellite imagery processing. Our methods are efficient and do not rely on the prior existence of complex inverse mapping functions to correlate 2D image coordinates and 3D terrain. Our first solver improves on the stereo correspondence problem for satellite imagery, in that we provide an exact image-to-object space mapping (where prior methods were inaccurate). Our second solver provides a novel mechanism for 3D point triangulation, which has improved robustness and accuracy over prior techniques. Given the usefulness and ubiquity of satellite imagery, our proposed methods allow for improved results in a variety of existing and future applications.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_3dim_HeinlyDF14.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_3dim_HeinlyDF14-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_3dim_HeinlyDF14-ref"></a>
</dt>
<dd> <span class="bold"> Recovering Correct Reconstructions from Indistinguishable Geometry</span> <br> <i> Jared Heinly,
Enrique Dunn,
Jan-Michael Frahm </i><br> International Conference on 3D Vision (3DV 2014)</dd>
<div class="modal fade modal-conf_3dim_HeinlyDF14-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/3dim/HeinlyDF14,
author = {Jared Heinly and
Enrique Dunn and
Jan-Michael Frahm},
title = {Recovering Correct Reconstructions from Indistinguishable Geometry},
booktitle = {2nd International Conference on 3D Vision, 3DV 2014, Tokyo, Japan,
December 8-11, 2014, Volume 1},
pages = {377--384},
year = {2014}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_3dim_HeinlyDF14-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
Structure-from-motion (SFM) is widely utilized to generate 3D reconstructions from unordered photo-collections. However, in the presence of non unique, symmetric, or otherwise indistinguishable structure, SFM techniques often incorrectly reconstruct the final model. We propose a method that not only determines if an error is present, but automatically corrects the error in order to produce a correct representation of the scene. We find that by exploiting the co-occurrence information present in the sceneâ€™s geometry, we can successfully isolate the 3D points causing the incorrect result. This allows us to split an incorrect reconstruction into error-free sub-models that we then correctly merge back together. Our experimental results show that our technique is efficient, robust to a variety of scenes, and outperforms existing methods.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_cvpr_WangWDF14.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_cvpr_WangWDF14-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_cvpr_WangWDF14-ref"></a>
</dt>
<dd> <span class="bold"> Stereo under Sequential Optimal Sampling: A Statistical Analysis
Framework for Search Space Reduction</span> <br> <i> Yilin Wang,
Ke Wang,
Enrique Dunn,
Jan-Michael Frahm </i><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2014)</dd>
<div class="modal fade modal-conf_cvpr_WangWDF14-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/cvpr/WangWDF14,
author = {Yilin Wang and
Ke Wang and
Enrique Dunn and
Jan-Michael Frahm},
title = {Stereo under Sequential Optimal Sampling: A Statistical Analysis
Framework for Search Space Reduction},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition,
{CVPR} 2014, Columbus, OH, USA, June 23-28, 2014},
pages = {485--492},
year = {2014}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_cvpr_WangWDF14-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
    We develop a sequential optimal sampling framework
for stereo disparity estimation by adapting the Sequential
Probability Ratio Test (SPRT) model. We operate over local
image neighborhoods by iteratively estimating single pixel
disparity values until sufficient evidence has been gathered
to either validate or contradict the current hypothesis regarding
local scene structure. The output of our sampling
is a set of sampled pixel positions along with a robust and
compact estimate of the set of disparities contained within
a given region. We further propose an efficient plane propagation
mechanism that leverages the pre-computed sampling
positions and the local structure model described by
the reduced local disparity set. Our sampling framework
is a general pre-processing mechanism aimed at reducing
computational complexity of disparity search algorithms by
ascertaining a reduced set of disparity hypotheses for each
pixel. Experiments demonstrate the effectiveness of the proposed
approach when compared to state of the art methods.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_cvpr_ZhengDJF14.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_cvpr_ZhengDJF14-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_cvpr_ZhengDJF14-ref"></a>
</dt>
<dd> <span class="bold"> PatchMatch Based Joint View Selection and Depthmap Estimation</span> <br> <i> Enliang Zheng,
Enrique Dunn,
Vladimir Jojic,
Jan-Michael Frahm </i><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2014)</dd>
<div class="modal fade modal-conf_cvpr_ZhengDJF14-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/cvpr/ZhengDJF14,
author = {Enliang Zheng and
Enrique Dunn and
Vladimir Jojic and
Jan-Michael Frahm},
title = {PatchMatch Based Joint View Selection and Depthmap Estimation},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition,
{CVPR} 2014, Columbus, OH, USA, June 23-28, 2014},
pages = {1510--1517},
year = {2014}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_cvpr_ZhengDJF14-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We propose a multi-view depthmap estimation approach aimed at adaptively ascertaining the pixel level data associations between a reference image and all the elements of a source image set. Namely, we address the question, what aggregation subset of the source image set should we use to estimate the depth of a particular pixel in the reference image? We pose the problem within a probabilistic framework that jointly models pixel-level view selection and depthmap estimation given the local pairwise image photoconsistency. The corresponding graphical model is solved by EM-based view selection probability inference and PatchMatch-like depth sampling and propagation. Experimental results on standard multi-view benchmarks convey the state-of-the art estimation accuracy afforded by mitigating spurious pixellevel data associations. Additionally, experiments on large Internet crowd sourced data demonstrate the robustness of our approach against unstructured and heterogeneous image capture characteristics. Moreover, the linear computational and storage requirements of our formulation, as well as its inherent parallelism, enables an efficient and scalable GPU-based implementation.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_eccv_HeinlyDF14.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_eccv_HeinlyDF14-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_eccv_HeinlyDF14-ref"></a>
</dt>
<dd> <span class="bold"> Correcting for Duplicate Scene Structure in Sparse 3D Reconstruction</span> <br> <i> Jared Heinly,
Enrique Dunn,
Jan-Michael Frahm </i><br>
European Conference on Computer Vision (ECCV 2014) </dd>
<div class="modal fade modal-conf_eccv_HeinlyDF14-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/eccv/HeinlyDF14,
author = {Jared Heinly and
Enrique Dunn and
Jan-Michael Frahm},
title = {Correcting for Duplicate Scene Structure in Sparse 3D Reconstruction},
booktitle = {Computer Vision - {ECCV} 2014 - 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part {IV}},
pages = {780--795},
year = {2014}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_eccv_HeinlyDF14-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
Structure from motion (SfM) is a common technique to recover 3D geometry and camera poses from sets of images of a common scene. In many urban environments, however, there are symmetric, repetitive, or duplicate structures that pose challenges for SfM pipelines. The result of these ambiguous structures is incorrectly placed cameras and points within the reconstruction. In this paper, we present a postprocessing method that can not only detect these errors, but successfully resolve them. Our novel approach proposes the strong and informative measure of conflicting observations, and we demonstrate that it is robust to a large variety of scenes.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_eccv_JiDF14.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_eccv_JiDF14-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_eccv_JiDF14-ref"></a>
</dt>
<dd> <span class="bold"> 3D Reconstruction of Dynamic Textures in Crowd Sourced Data</span> <br> <i> Dinghuang Ji,
Enrique Dunn,
Jan-Michael Frahm </i><br> European Conference on Computer Vision (ECCV 2014)</dd>
<div class="modal fade modal-conf_eccv_JiDF14-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/eccv/JiDF14,
author = {Dinghuang Ji and
Enrique Dunn and
Jan-Michael Frahm},
title = {3D Reconstruction of Dynamic Textures in Crowd Sourced Data},
booktitle = {Computer Vision - {ECCV} 2014 - 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part {I}},
pages = {143--158},
year = {2014}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_eccv_JiDF14-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We propose a framework to automatically build 3D models for scenes containing structures not amenable for photo-consistency based reconstruction due to having dynamic appearance. We analyze the dynamic appearance elements of a given scene by leveraging the imagery contained in Internet image photo-collections and online video sharing websites. Our approach combines large scale crowd sourced SfM techniques with image content segmentation and shape from silhouette techniques to build an iterative framework for 3D shape estimation. The developed system not only enables more complete and robust 3D modeling, but it also enables more realistic visualizations through the identifi- cation of dynamic scene elements amenable to dynamic texture mapping. Experiments on crowd sourced image and video datasets illustrate the effectiveness of our automated data-driven approach.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_eccv_ZhengWDF14.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_eccv_ZhengWDF14-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_eccv_ZhengWDF14-ref"></a>
</dt>
<dd> <span class="bold"> Joint Object Class Sequencing and Trajectory Triangulation (JOST)</span> <br> <i> Enliang Zheng,
Ke Wang,
Enrique Dunn,
Jan-Michael Frahm </i><br> European Conference on Computer Vision (ECCV 2014)</dd>
<div class="modal fade modal-conf_eccv_ZhengWDF14-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/eccv/ZhengWDF14,
author = {Enliang Zheng and
Ke Wang and
Enrique Dunn and
Jan-Michael Frahm},
title = {Joint Object Class Sequencing and Trajectory Triangulation (JOST)},
booktitle = {Computer Vision - {ECCV} 2014 - 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part {VII}},
pages = {599--614},
year = {2014}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_eccv_ZhengWDF14-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We introduce the problem of joint object class sequencing and trajectory triangulation (JOST), which is defined as the reconstruction of the motion path of a class of dynamic objects through a scene from an unordered set of images. We leverage standard object detection techinques to identify object instances within a set of registered images. Each of these object detections defines a single 2D point with a corresponding viewing ray. The set of viewing rays attained from the aggregation of all detections belonging to a common object class is then used to estimate a motion path denoted as the object class trajectory. Our method jointly determines the topology of the objects motion path and reconstructs the 3D object points corresponding to our object detections. We pose the problem as an optimization over both the unknown 3D points and the topology of the path, which is approximated by a Generalized Minimum Spanning Tree (GMST) on a multipartite graph and then refined through a continuous optimization over the 3D object points. Experiments on synthetic and real datasets demonstrate the effectiveness of our method and the feasibility to solve a previously intractable problem.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_ismar_MeshramMYDFM14.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_ismar_MeshramMYDFM14-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_ismar_MeshramMYDFM14-ref"></a>
</dt>
<dd> <span class="bold"> P-HRTF: Efficient personalized HRTF computation for high-fidelity spatial sound</span> <br> <i> Alok Meshram,
Ravish Mehra,
Hongsheng Yang,
Enrique Dunn,
Jan-Michael Frahm,
Dinesh Manocha </i><br> IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2014)</dd>
<div class="modal fade modal-conf_ismar_MeshramMYDFM14-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/ismar/MeshramMYDFM14,
author = {Alok Meshram and
Ravish Mehra and
Hongsheng Yang and
Enrique Dunn and
Jan-Michael Frahm and
Dinesh Manocha},
title = {P-HRTF: Efficient personalized HRTF computation for high-fidelity spatial sound},
booktitle = {IEEE International Symposium on Mixed and Augmented Reality, {ISMAR}
2014, Munich, Germany, September 10-12, 2014},
pages = {53--61},
year = {2014}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_ismar_MeshramMYDFM14-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
Accurate rendering of 3D spatial audio for interactive virtual auditory displays requires the use of personalized head-related transfer functions (HRTFs). We present a new approach to compute personalized HRTFs for any individual using a method that combines state-of-the-art image-based 3D modeling with an efficient numerical simulation pipeline. Our 3D modeling framework enables capture of the listenerâ€™s head and torso using consumer-grade digital cameras to estimate a high-resolution non-parametric surface representation of the head, including the extended vicinity of the listenerâ€™s ear. We leverage sparse structure from motion and dense surface reconstruction techniques to generate a 3D mesh. This mesh is used as input to a numeric sound propagation solver, which uses acoustic reciprocity and Kirchhoff surface integral representation to efficiently compute an individualâ€™s personalized HRTF. The overall computation takes tens of minutes on multi-core desktop machine. We have used our approach to compute the personalized HRTFs of few individuals, and we present our preliminary evaluation here. To the best of our knowledge, this is the first commodity technique that can be used to compute personalized HRTFs in a lab or home setting
</div>
</div>
</div>
</div>
</div>
<dt> <a href="http://ieeexplore.ieee.org/document/6836006/" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_wacv_ChoDF14-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_wacv_ChoDF14-ref"></a>
</dt>
<dd> <span class="bold"> Rotation estimation from cloud tracking</span> <br> <i> Sangwoo Cho,
Enrique Dunn,
Jan-Michael Frahm </i><br> IEEE Winter Conference on Applications of Computer Vision (WACV 14)</dd>
<div class="modal fade modal-conf_wacv_ChoDF14-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/wacv/ChoDF14,
author = {Sangwoo Cho and
Enrique Dunn and
Jan-Michael Frahm},
title = {Rotation estimation from cloud tracking},
booktitle = {IEEE Winter Conference on Applications of Computer Vision, Steamboat
Springs, CO, USA, March 24-26, 2014},
pages = {917--924},
year = {2014}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_wacv_ChoDF14-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We address the problem of online relative orientation estimation from streaming video captured by a sky-facing camera on a mobile device. Namely, we rely on the detection and tracking of visual features attained from cloud structures. Our proposed method achieves robust and efficient operation by combining realtime visual odometry modules, learning based feature classification, and Kalman filtering within a robustness-driven data management framework, while achieving framerate processing on a mobile device. The relatively large 3D distance between the camera and the observed cloud features is leveraged to simplify our processing pipeline. First, as an efficiency driven optimization, we adopt a homography based motion model and focus on estimating relative rotations across adjacent keyframes. To this end, we rely on efficient feature extraction, KLT tracking, and RANSAC based model fitting. Second, to ensure the validity of our simplified motion model, we segregate detected cloud features from scene features through SVM classification. Finally, to make tracking more robust, we employ predictive Kalman filtering to enable feature persistence through temporary occlusions and manage feature spatial distribution to foster tracking robustness. Results exemplify the accuracy and robustness of the proposed approach and highlight its potential as a passive orientation sensor.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="https://www.cs.unc.edu/~kewang/assets/wacv2014.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_wacv_WangDTF14-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_wacv_WangDTF14-ref"></a>
</dt>
<dd> <span class="bold"> Combining semantic scene priors and haze removal for single image
depth estimation</span> <br> <i> Ke Wang,
Enrique Dunn,
Joseph Tighe,
Jan-Michael Frahm </i><br> IEEE Winter Conference on Applications of Computer Vision (WACV 2014)</dd>
<div class="modal fade modal-conf_wacv_WangDTF14-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/wacv/WangDTF14,
author = {Ke Wang and
Enrique Dunn and
Joseph Tighe and
Jan-Michael Frahm},
title = {Combining semantic scene priors and haze removal for single image
depth estimation},
booktitle = {IEEE Winter Conference on Applications of Computer Vision, Steamboat
Springs, CO, USA, March 24-26, 2014},
pages = {800--807},
year = {2014}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_wacv_WangDTF14-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We consider the problem of estimating the relative depth of a scene from a monocular image. The dark channel prior, used as a statistical observation of haze free images, has been previously leveraged for haze removal and relative depth estimation tasks. However, as a local measure, it fails to account for higher order semantic relationship among scene elements. We propose a dual channel prior used for identifying pixels that are unlikely to comply with the dark channel assumption, leading to erroneous depth estimates. We further leverage semantic segmentation information and patch match label propagation to enforce semantically consistent geometric priors. Experiments illustrate the quantitative and qualitative advantages of our approach when compared to state of the art methods.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_3dim_WangDF12.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_3dim_WangDF12-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_3dim_WangDF12-ref"></a>
</dt>
<dd> <span class="bold"> Increasing the Efficiency of Local Stereo by Leveraging Smoothness
Constraints</span> <br> <i> Yilin Wang,
Enrique Dunn,
Jan-Michael Frahm </i><br> International Conference on 3D Imaging, Modeling, Processing, Visualization & Transmission (3DIMPVT 2012)</dd>
<div class="modal fade modal-conf_3dim_WangDF12-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/3dim/WangDF12,
author = {Yilin Wang and
Enrique Dunn and
Jan-Michael Frahm},
title = {Increasing the Efficiency of Local Stereo by Leveraging Smoothness
Constraints},
booktitle = {2012 Second International Conference on 3D Imaging, Modeling, Processing,
Visualization & Transmission, Zurich, Switzerland, October 13-15,
2012},
pages = {246--253},
year = {2012}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_3dim_WangDF12-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We introduce a novel framework for efficient stereo disparity estimation leveraging the spatial smoothness typically assumed in stereo and formalized by the various smoothness constraints. The smoothness constraint presumes that a neighboring set of pixels shares the same disparity or the disparity varies smoothly. Our key insight is that it hence suffices to evaluate any single one of those pixels at the correct disparity to identify a valid estimate for the entire set. We leverage this insight into the formulation of a complexity reducing mechanism. We distribute the exploration of the disparity search space among neighboring pixels, effectively reducing the set of disparity hypothesis evaluated at each individual pixel. Moreover, we integrate a recently proposed concept to deploy sparsity within this neighborhood of distributed disparities into our novel mechanism, in order to further reduce the computational burden. Our experiments clearly demonstrate the effectiveness of our approach by achieving comparable results to the baseline of exhaustive disparity search. The analysis of the computational complexity of our proposed mechanisms illustrates how, by making moderate assumptions on the smoothness of the observed scene, we can reduce the computational complexity of local stereo disparity search by upwards of two orders of magnitude while maintaining the comparable result quality.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_bmvc_ZhengDRF12.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_bmvc_ZhengDRF12-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_bmvc_ZhengDRF12-ref"></a>
</dt>
<dd> <span class="bold"> Efficient and Scalable Depthmap Fusion</span> <br> <i> Enliang Zheng,
Enrique Dunn,
Rahul Raguram,
Jan-Michael Frahm </i><br> British Machine Vision Conference (BMVC 2012)</dd>
<div class="modal fade modal-conf_bmvc_ZhengDRF12-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/bmvc/ZhengDRF12,
author = {Enliang Zheng and
Enrique Dunn and
Rahul Raguram and
Jan-Michael Frahm},
title = {Efficient and Scalable Depthmap Fusion},
booktitle = {British Machine Vision Conference, {BMVC} 2012, Surrey, UK, September
3-7, 2012},
pages = {1--12},
year = {2012}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_bmvc_ZhengDRF12-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
The estimation of a complete 3D model from a set of depthmaps is a data intensive task aimed at mitigating measurement noise in the input data by leveraging the inherent redundancy in overlapping multi-view observations. In this paper we propose an effi- cient depthmap fusion approach that reduces the memory complexity associated with volumetric scene representations. By virtue of reducing the memory footprint we are able to process an increased reconstruction volume with greater spatial resolution. Our approach also improves upon state of the art fusion techniques by approaching the problem in an incremental online setting instead of batch mode processing. In this way, are able to handle an arbitrary number of input images at high pixel resolution and facilitate a streaming 3D processing pipeline. Experiments demonstrate the effectiveness of our proposal both at 3D modeling from internet-scale crowd source data as well as close-range 3D modeling from high resolution video streams
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_eccv_HeinlyDF12.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_eccv_HeinlyDF12-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_eccv_HeinlyDF12-ref"></a>
</dt>
<dd> <span class="bold"> Comparative Evaluation of Binary Features</span> <br> <i> Jared Heinly,
Enrique Dunn,
Jan-Michael Frahm </i><br> European Conference on Computer Vision (ECCV 2012) </dd>
<div class="modal fade modal-conf_eccv_HeinlyDF12-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/eccv/HeinlyDF12,
author = {Jared Heinly and
Enrique Dunn and
Jan-Michael Frahm},
title = {Comparative Evaluation of Binary Features},
booktitle = {Computer Vision - {ECCV} 2012 - 12th European Conference on Computer
Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part {II}},
pages = {759--773},
year = {2012}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_eccv_HeinlyDF12-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
Performance evaluation of salient features has a long-standing tradition in computer vision. In this paper, we fill the gap of evaluation for the recent wave of binary feature descriptors, which aim to provide robustness while achieving high computational efficiency. We use established metrics to embed our assessment into the body of existing evaluations, allowing us to provide a novel taxonomy unifying both traditional and novel binary features. Moreover, we analyze the performance of different detector and descriptor pairings, which are often used in practice but have been infrequently analyzed. Additionally, we complement existing datasets with novel data testing for illumination change, pure camera rotation, pure scale change, and the variety present in photo-collections. Our performance analysis clearly demonstrates the power of the new class of features. To benefit the community, we also provide a website for the automatic testing of new description methods using our provided metrics and datasets (www.cs.unc.edu/feature-evaluation).
</div>
</div>
</div>
</div>
</div>
<dt> <a href="https://link.springer.com/chapter/10.1007%2F978-3-642-31519-0_5" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_evolve_NaredoDT12-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_evolve_NaredoDT12-ref"></a>
</dt>
<dd> <span class="bold"> Disparity Map Estimation by Combining Cost Volume Measures Using Genetic
Programming</span> <br> <i> Enrique Naredo,
Enrique Dunn,
Leonardo Trujillo </i><br>
EVOLVE - A Bridge between Probability, Set Oriented Numerics,
and Evolutionary Computation (EVOLVE 2012)</dd>
<div class="modal fade modal-conf_evolve_NaredoDT12-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/evolve/NaredoDT12,
author = {Enrique Naredo and
Enrique Dunn and
Leonardo Trujillo},
title = {Disparity Map Estimation by Combining Cost Volume Measures Using Genetic
Programming},
booktitle = {{EVOLVE} - {A} Bridge between Probability, Set Oriented Numerics,
and Evolutionary Computation II, {EVOLVE} 2012, Mexico City, Mexico,
August 7-9, 2012, Proceedings},
pages = {71--86},
year = {2012}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_evolve_NaredoDT12-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
Stereo vision is one of the most active research areas in modern computer vision. The objective is to recover 3-D depth information from a pair of 2-D images that capture the same scene. This paper addresses the problem of dense stereo correspondence, where the goal is to determine which image pixels in both images are projections of the same 3-D point from the observed scene. The proposal in this work is to build a non-linear operator that combines three well known methods to derive a correspondence measure that allows us to retrieve a better approximation of the ground truth disparity of stereo image pair. To achieve this, the problem is posed as a search and optimization task and solved with genetic programming (GP), an evolutionary paradigm for automatic program induction. Experimental results on well known benchmark problems show that the combined correspondence measure produced by GP outperforms each standard method, based on the mean error and the percentage of bad pixels. In conclusion, this paper shows that GP can be used to build composite correspondence algorithms that exhibit a strong performance on standard tests.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="http://ieeexplore.ieee.org/document/5955360/" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_3dim_WangDF11-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_3dim_WangDF11-ref"></a>
</dt>
<dd> <span class="bold"> An Approach for Shape from Surface Normals with Local Discontinuity
Detection</span> <br> <i> Yilin Wang,
Enrique Dunn,
Jan-Michael Frahm </i><br>
International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT 2011)</dd>
<div class="modal fade modal-conf_3dim_WangDF11-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/3dim/WangDF11,
author = {Yilin Wang and
Enrique Dunn and
Jan-Michael Frahm},
title = {An Approach for Shape from Surface Normals with Local Discontinuity
Detection},
booktitle = {International Conference on 3D Imaging, Modeling, Processing, Visualization
and Transmission, 3DIMPVT 2011, Hangzhou, China, 16-19 May 2011},
pages = {188--195},
year = {2011}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_3dim_WangDF11-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We present a multi-modal surface reconstruction approach, which utilizes direct surface orientation measurements along with luminance information to obtain high quality 3D reconstructions. The proposed approach models local surface geometry as a set of intersecting natural cubic splines estimated through least squares fitting of our input pixel-wise surface normal measurements. We use this representation to detect discontinuities and segment our scene into disjoint continuous surfaces, which are constructed by an aggregation of connected local surface geometry elements. In order to obtain absolute depth estimates, we introduce the concept of multi-view patch sweeping, where we search for the most photo-consistent patch displacement along a viewing ray. Our approach improves on existing shape from normals methods by enabling absolute depth estimates for scenes with multiple objects. Furthermore, in contrast to existing multi-view stereo methods, we are able to reconstruct textureless regions through the propagation of relative surface orientation measurements. Experiments on synthetic and real data are presented to validate our proposal.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_bmvc_JenDGF11.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_bmvc_JenDGF11-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_bmvc_JenDGF11-ref"></a>
</dt>
<dd> <span class="bold"> Adaptive Scale Selection for Hierarchical Stereo</span> <br> <i> Yi-Hung Jen,
Enrique Dunn,
Pierre Fite Georgel,
Jan-Michael Frahm </i><br> British Machine Vision Conference (BMVC 2011)</dd>
<div class="modal fade modal-conf_bmvc_JenDGF11-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/bmvc/JenDGF11,
author = {Yi-Hung Jen and
Enrique Dunn and
Pierre Fite Georgel and
Jan-Michael Frahm},
title = {Adaptive Scale Selection for Hierarchical Stereo},
booktitle = {British Machine Vision Conference, {BMVC} 2011, Dundee, UK, August
29 - September 2, 2011. Proceedings},
pages = {1--10},
year = {2011}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_bmvc_JenDGF11-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
Hierarchical stereo provides an efficient coarse-to-fine mechanism for disparity map estimation. However, common drawbacks of such an approach include the loss of high frequency structures not observable at coarse scale levels, as well as the unrecoverable propagation of erroneous disparity estimates through the scale space. This paper presents an adaptive scale selection mechanism to determine a suitable resolution level from which to begin the hierarchical depth estimation process for each pixel. The proposed scale selection mechanism allows us to robustly implement variable cost aggregation in order to reduce the variability of the photo-consistency measure across scale space. We also incorporate a weighted shiftable window mechanism to enable error correction during coarse-to-fine depth refinement. Experiments illustrate the effectiveness of our approach in terms of disparity accuracy, while attaining a computational efficiency compromise between full resolution and hierarchical disparity map estimation.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="http://ieeexplore.ieee.org/document/6126368/" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_iccv_DunnCF11-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_iccv_DunnCF11-ref"></a>
</dt>
<dd> <span class="bold"> A geometric solver for calibrated stereo egomotion</span> <br> <i> Enrique Dunn,
Brian Clipp,
Jan-Michael Frahm </i><br> IEEE International Conference on Computer Vision (ICCV 2011)</dd>
<div class="modal fade modal-conf_iccv_DunnCF11-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/iccv/DunnCF11,
author = {Enrique Dunn and
Brian Clipp and
Jan-Michael Frahm},
title = {A geometric solver for calibrated stereo egomotion},
booktitle = {IEEE International Conference on Computer Vision, {ICCV} 2011, Barcelona,
Spain, November 6-13, 2011},
pages = {1187--1194},
year = {2011}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_iccv_DunnCF11-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
This paper introduces a novel geometrical solution for the pose estimation of a stereo camera system as commonly used in robotics, where the camera system balances between coverage and overlap. The proposed approach considers a set of features observed, respectively, in four, three and two views. In contrast to most algebraic solutions our constraints are geometrically meaningful. Initially, we use a four view feature to restrict our translation vector to lie on the surface of a sphere while setting orientation as a function of translation up to a single rotational degree of freedom. Next, we use a three view feature to restrict the translation vector to lie on a circle on the sphere, while completely defining orientation as a function of translation. Finally, we use a two view feature to determine the translation vector lying on the intersection of the circle and one of the generator lines of a doubly ruled quadric. We show how for this final step, the problem can be reduced to the intersection of two coplanar circles. We also analyze the degenerate configurations of the proposed solver and perform an experimental evaluation.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_eccv_FrahmGGJRWJDCL10.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_eccv_FrahmGGJRWJDCL10-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_eccv_FrahmGGJRWJDCL10-ref"></a>
</dt>
<dd> <span class="bold"> Building Rome on a Cloudless Day</span> <br> <i> Jan-Michael Frahm,
Pierre Fite Georgel,
David Gallup,
Tim Johnson,
Rahul Raguram,
Changchang Wu,
Yi-Hung Jen,
Enrique Dunn,
Brian Clipp,
Svetlana Lazebnik </i><br> European Conference on Computer Vision (ECCV 2010)</dd>
<div class="modal fade modal-conf_eccv_FrahmGGJRWJDCL10-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/eccv/FrahmGGJRWJDCL10,
author = {Jan-Michael Frahm and
Pierre Fite Georgel and
David Gallup and
Tim Johnson and
Rahul Raguram and
Changchang Wu and
Yi-Hung Jen and
Enrique Dunn and
Brian Clipp and
Svetlana Lazebnik},
title = {Building Rome on a Cloudless Day},
booktitle = {Computer Vision - {ECCV} 2010, 11th European Conference on Computer
Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings,
Part {IV}},
pages = {368--381},
year = {2010}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_eccv_FrahmGGJRWJDCL10-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
This paper introduces an approach for dense 3D reconstruc- 7 8 tion from unregistered Internet-scale photo collections with about 3 mil- 8 9 lion of images within the span of a day on a single PC (â€œcloudlessâ€). Our 9 10 method advances image clustering, stereo, stereo fusion and structure 10 11 from motion to achieve high computational performance. We leverage 11 12 geometric and appearance constraints to obtain a highly parallel imple- 12 13 mentation on modern graphics processors and multi-core architectures. 13 14 This leads to two orders of magnitude higher performance on an order 14 15 of magnitude larger dataset than competing state-of-the-art approaches.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="http://ieeexplore.ieee.org/document/5354179/?reload=true&arnumber=5354179" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_bmvc_DunnF09-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_bmvc_DunnF09-ref"></a>
</dt>
<dd> <span class="bold"> Next Best View Planning for Active Model Improvement</span> <br> <i> Enrique Dunn,
Jan-Michael Frahm </i><br> British Machine Vision Conference, {BMVC} 2009</dd>
<div class="modal fade modal-conf_bmvc_DunnF09-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/bmvc/DunnF09,
author = {Enrique Dunn and
Jan-Michael Frahm},
title = {Next Best View Planning for Active Model Improvement},
booktitle = {British Machine Vision Conference, {BMVC} 2009, London, UK, September
7-10, 2009. Proceedings},
pages = {1--11},
year = {2009}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_bmvc_DunnF09-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We propose a novel approach to determining the Next Best View (NBV) for the task of efficiently building highly accurate 3D models from images. Our proposed method deploys a hierarchical uncertainty driven model refinement process designed to select vantage viewpoints based on the modelâ€™s covariance structure and appearance, as well as the camera characteristics. The developed NBV planning system incrementally builds a sensing strategy by sequentially finding the single camera placement, which best reduces an existing modelâ€™s 3D uncertainty. The generic nature of our systemâ€™s design and internal data representation makes it well suited to be applied to a wide variety of 3D modeling algorithms. It can be used within active computer vision systems as well as for optimized view selection from the set of available views. Experimental results are presented to illustrate the effectiveness and versatility of our approach.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="pubs/conf_iros_DunnBF09.pdf" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-conf_iros_DunnBF09-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-conf_iros_DunnBF09-ref"></a>
</dt>
<dd> <span class="bold"> Developing visual sensing strategies through next best view planning</span> <br> <i> Enrique Dunn,
Jur P. van den Berg,
Jan-Michael Frahm </i><br> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2009)</dd>
<div class="modal fade modal-conf_iros_DunnBF09-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@inproceedings{DBLP:conf/iros/DunnBF09,
author = {Enrique Dunn and
Jur P. van den Berg and
Jan-Michael Frahm},
title = {Developing visual sensing strategies through next best view planning},
booktitle = {2009 {IEEE/RSJ} International Conference on Intelligent Robots and
Systems, October 11-15, 2009, St. Louis, MO, {USA}},
pages = {4001--4008},
year = {2009}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-conf_iros_DunnBF09-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
We propose an approach for acquiring geometric 3D models using cameras mounted on autonomous vehicles and robots. Our method uses structure from motion techniques from computer vision to obtain the geometric structure of the scene. To achieve an efficient goal-driven resource deployment, we develop an incremental approach, which alternates between an accuracy-driven next best view determination and recursive path planning. The next best view is determined by a novel cost function that quantifies the expected contribution of future viewing configurations. A sensing path for robot motion towards the next best view is then achieved by a cost-driven recursive search of intermediate viewing configurations. We discuss some of the properties of our view cost function in the context of an iterative view planning process and present experimental results on a synthetic environment.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="https://link.springer.com/chapter/10.1007/978-3-540-79438-7_8" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-series_sci_OlagueDL08-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-series_sci_OlagueDL08-ref"></a>
</dt>
<dd> <span class="bold"> Individual Evolution as an Adaptive Strategy for Photogrammetric Network
Design</span> <br> <i> Gustavo Olague,
Enrique Dunn,
Evelyne Lutton </i><br> Adaptive and Multilevel Metaheuristics</dd>
<div class="modal fade modal-series_sci_OlagueDL08-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@incollection{DBLP:series/sci/OlagueDL08,
author = {Gustavo Olague and
Enrique Dunn and
Evelyne Lutton},
title = {Individual Evolution as an Adaptive Strategy for Photogrammetric Network
Design},
booktitle = {Adaptive and Multilevel Metaheuristics},
pages = {157--176},
year = {2008}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-series_sci_OlagueDL08-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
This chapter introduces individual evolution as a strategy for problem solving. This strategy proposes to partition the original problem into a set of homogeneous elements, whose individual contribution to the problem solution can be evaluated separately. A population comprised of these homogeneous elements is evolved with the goal of creating a single solution by a process of aggregation. The goal of individual evolution is to locally build better individuals that jointly form better global solutions. The implementation of the proposed approach requires addressing aspects such as problem decomposition and representation, local and global fitness integration, as well as diversity preservation mechanisms. The benefit of applying the individual evolution approach for problem solving is a substantial reduction in computational effort expended in the evolutionary optimization process. This chapter shows an example from vision metrology where experimental results coincide with previous state of the art photogrammetric network design methodologies, while incurring in only a fraction of the computational cost.
</div>
</div>
</div>
</div>
</div>
<dt> <a href="http://www.sciencedirect.com/science/article/pii/S016786550500334X?via%3Dihub" class="icon icon-basic-download" ></a>
<a class="icon icon-basic-message-txt" data-toggle="modal" data-target=".modal-journals_prl_DunnOL06-abs"></a>
<a class="icon-basic-info" data-toggle="modal" data-target=".modal-journals_prl_DunnOL06-ref"></a>
</dt>
<dd> <span class="bold"> Parisian camera placement for vision metrology</span> <br> <i> Enrique Dunn,
Gustavo Olague,
Evelyne Lutton </i><br> Pattern Recognition Letters</dd>
<div class="modal fade modal-journals_prl_DunnOL06-ref bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">BibTex</h4>
</div>
<div class="modal-body">
@article{DBLP:journals/prl/DunnOL06,
author = {Enrique Dunn and
Gustavo Olague and
Evelyne Lutton},
title = {Parisian camera placement for vision metrology},
journal = {Pattern Recognition Letters},
volume = {27},
number = {11},
pages = {1209--1219},
year = {2006}
}
</div>
</div>
</div>
</div>
</div>
<div class="modal fade modal-journals_prl_DunnOL06-abs bootstrap-modal" tabindex="-1" role="dialog" aria-labelledby="myLargeModalLabel" aria-hidden="true">
<div class="modal-dialog modal-lg">
<div class="modal-body">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
<h4 class="modal-title" id="myLargeModalLabel">Abstract</h4>
</div>
<div class="modal-body">
This paper presents a novel camera network design methodology based on the Parisian evolutionary computation approach. This methodology proposes to partition the original problem into a set of homogeneous elements, whose individual contribution to the problem solution can be evaluated separately. A population comprised of these homogeneous elements is evolved with the goal of creating a single solution by a process of aggregation. The goal of the Parisian evolutionary process is to locally build better individuals that jointly form better global solutions. The implementation of the proposed approach requires addressing aspects such as problem decomposition and representation, local and global fitness integration, as well as diversity preservation mechanisms. The benefit of applying the Parisian approach to our camera placement problem is a substantial reduction in computational effort expended in the evolutionary optimization process. Moreover, experimental results coincide with previous state of the art photogrammetric network design methodologies, while incurring in only a fraction of the computational cost.
</div>
</div>
</div>
</div>
</div>




<!--     myend references                   -->




                </dl>
            </div>




                       <div id="bio-link" class="mb-50">
              <h2 class="section-title pr-0">CURRICULUM <span class="bold">VITAE</span>
<!--                  <a href="blog-right-sidebar.html" class="section-more right">BLOG</a>-->
                  </h2>
            </div>

<!--            <div class="row">  -->
               <div class="mb-50">
<!--              <h2 class="section-title pr-0">Welcome... <span class="bold"></span>-->
<!--                  <a href="blog-right-sidebar.html" class="section-more right">BLOG</a>-->
<!--                  </h2>-->
                Enrique Dunn joined Stevens' Department of Computer Science as an Associate Professor in August 2016.
                Dr. Dunn earned a degree in computer engineering from the Autonomous University of Baja California (Mexico) in 1999. He completed a Masters degree in Computer Science in 2001 and a Doctorate in Science in Electronics and Telecommunications in 2006, both from the Ensenada Center for Scientific Research and Higher Education (Mexico). During his doctorate studies, Dr. Dunn carried out research while visiting the French Institute for Research in Computer Science and Control in Rocquencourt. He joined the Department of Computer Science of the University of North Carolina at Chapel Hill  as a visiting scholar in 2008, after being awarded a one year Postdoctoral Fellowship for Studies Abroad by the National Council for Science and Technology (Mexico). He remained with UNC-CH CS Department as a postdoctoral researcher until he became a Research Assistant Professor in 2012. Dr. Dunn has authored over 40 papers in international conferences and journals. He is a member of the Editorial Board of Elsevier Journal of Image and Vision Computing.
                </div>

            </div>


        </div>

          <!-- CONTACT INFO SECTION 1 -->
          <div id="contact-link" class="page-section p-80-cont grey-light-bg">
            <div class="container">
              <div class="row">

                <div class="col-md-4 col-sm-6">
                  <div class="cis-cont">
                    <div class="cis-icon">
                      <div class="icon icon-basic-map"></div>
                    </div>
                    <div class="cis-text">
                      <h3><span class="bold">ADDRESS</span></h3>
                      <p>1 Castle Point Terrace <br>North Bldg. Office 219</p>

                    </div>
                  </div>
                </div>
                <div class="col-md-4 col-sm-6">
                  <div class="cis-cont">
                    <div class="cis-icon">
                      <div class="icon icon-basic-mail"></div>
                    </div>
                    <div class="cis-text">
                      <h3><span class="bold">EMAIL</span></h3>
                      <p><a href="">edunn[AT]stevens.edu</a></p>
                    </div>
                  </div>
                </div>
                <div class="col-md-4 col-sm-6">
                  <div class="cis-cont">
                    <div class="cis-icon">
                      <div class="icon icon-basic-smartphone"></div>
                    </div>
                    <div class="cis-text">
                      <h3><span class="bold">PHONE</span></h3>
                      <p>201.216.3382</p>
                    </div>
                  </div>
                </div>

              </div>
            </div>
          </div>




				<!-- BACK TO TOP -->
				<p id="back-top">
          <a href="#top" title="Back to Top"><span class="icon icon-arrows-up"></span></a>
        </p>

			</div><!-- End BG -->
		</div><!-- End wrap -->

<!-- JS begin -->

		<!-- jQuery  -->
		<script type="text/javascript" src="js/jquery-1.11.2.min.js"></script>

		<!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>

		<!-- MAGNIFIC POPUP -->
		<script src='js/jquery.magnific-popup.min.js'></script>

    <!-- PORTFOLIO SCRIPTS -->
    <script type="text/javascript" src="js/isotope.pkgd.min.js"></script>
    <script type="text/javascript" src="js/imagesloaded.pkgd.min.js"></script>
    <script type="text/javascript" src="js/masonry.pkgd.min.js"></script>

    <!-- COUNTER -->
    <script type="text/javascript" src="js/jquery.countTo.js"></script>

    <!-- APPEAR -->
    <script type="text/javascript" src="js/jquery.appear.js"></script>

    <!-- OWL CAROUSEL -->
    <script type="text/javascript" src="js/owl.carousel.min.js"></script>

    <!-- PARALLAX -->
    <script type="text/javascript" src="js/jquery.stellar.min.js"></script>

    <!-- ONE PAGE NAV -->
    <script src="js/jquery.nav.js"></script>
    <script type="text/javascript">
      $(document).ready(function() {
      //ONE PAGE NAV	---------------------------------------------------------------------------
      var top_offset = $('header').height() - 1;  // get height of fixed navbar

      $('#nav-onepage').onePageNav({
        currentClass: 'current',
        changeHash: false,
        scrollSpeed: 700,
        scrollOffset: top_offset,
        scrollThreshold: 0.5,
        filter: '',
        easing: 'swing',
        begin: function() {
          //I get fired when the animation is starting
        },
        end: function() {
          //I get fired when the animation is ending
        },
        scrollChange: function($currentListItem) {
          //I get fired when you enter a section and I pass the list item of the section
        }
      });

      });//END document.ready
    </script>

    <!-- GOOLE MAP -->
    <script type="text/javascript" src="https://maps.google.com/maps/api/js?key=AIzaSyDzf6Gmc9u7rr2JHijOERAmC_j0gWYtR2c"></script>
    <script type="text/javascript" src="js/gmap3.min.js"></script>

    <!-- FORMS VALIDATION	-->
		<script src="js/jquery.validate.min.js"></script>
		<script src="js/contact-form-validation.min.js"></script>

    <!-- MAIN SCRIPT -->
    <script src="js/main.js"></script>

<!-- JS end -->

	</body>
</html>
